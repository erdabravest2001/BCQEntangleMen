{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Nucleus to create a dataset for QCNN\n",
    "Code modified from [this work](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "!gsutil cp gs://deepvariant/case-study-testdata/NA12878_sliced.bam NA12878_sliced.bam\n",
    "!gsutil cp gs://deepvariant/case-study-testdata/NA12878_sliced.bam.bai NA12878_sliced.bam.bai\n",
    "\n",
    "!wget ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz -O NA12878_calls.vcf.gz\n",
    "!wget ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz.tbi -O NA12878_calls.vcf.gz.tbi\n",
    "\n",
    "!gsutil cp gs://deepvariant/case-study-testdata/hs37d5.fa.gz hs37d5.fa.gz\n",
    "!gsutil cp gs://deepvariant/case-study-testdata/hs37d5.fa.gz.fai hs37d5.fa.gz.fai\n",
    "!gsutil cp gs://deepvariant/case-study-testdata/hs37d5.fa.gz.gzi hs37d5.fa.gz.gzi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run this we need pip==0.21.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q google-nucleus==0.5.6\n",
    "!pip install -q tensorflow==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nucleus.io import fasta\n",
    "from nucleus.io import sam\n",
    "from nucleus.io import vcf\n",
    "from nucleus.io.genomics_writer import TFRecordWriter\n",
    "from nucleus.protos import reads_pb2\n",
    "from nucleus.util import cigar\n",
    "from nucleus.util import ranges\n",
    "from nucleus.util import utils\n",
    "\n",
    "# Import TensorFlow after Nucleus.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ALLOWED_CIGAR_OPS = frozenset([cigar.CHAR_TO_CIGAR_OPS[op] for op in 'MX='])\n",
    "_ALLOWED_BASES = 'ACGT'\n",
    "_TRAIN = 'train.tfrecord'\n",
    "_EVAL = 'eval.tfrecord'\n",
    "_TEST = 'test.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfrecord_datasets(hparams):\n",
    "  \"\"\"Writes out TFRecords files for training, evaluation, and test datasets.\"\"\"\n",
    "  if not os.path.exists(hparams.out_dir):\n",
    "    os.makedirs(hparams.out_dir)\n",
    "\n",
    "  # Fraction of examples in each dataset.\n",
    "  train_eval_test_split = [0.7, 0.2, 0.1]\n",
    "  num_train_examples = 0\n",
    "  num_eval_examples = 0\n",
    "  num_test_examples = 0\n",
    "\n",
    "  # Generate training, test, and evaluation examples.\n",
    "  with TFRecordWriter(os.path.join(hparams.out_dir, _TRAIN)) as train_out, \\\n",
    "       TFRecordWriter(os.path.join(hparams.out_dir, _EVAL)) as eval_out, \\\n",
    "       TFRecordWriter(os.path.join(hparams.out_dir, _TEST)) as test_out:\n",
    "    all_examples = make_ngs_examples(hparams)\n",
    "    for example in all_examples:\n",
    "      r = random.random()\n",
    "      if r < train_eval_test_split[0]:\n",
    "        train_out.write(proto=example)\n",
    "        num_train_examples += 1\n",
    "      elif r < train_eval_test_split[0] + train_eval_test_split[1]:\n",
    "        eval_out.write(proto=example)\n",
    "        num_eval_examples += 1\n",
    "      else:\n",
    "        test_out.write(proto=example)\n",
    "        num_test_examples += 1\n",
    "  print('# of training examples: %d' % num_train_examples)\n",
    "  print('# of evaluation examples: %d' % num_eval_examples)\n",
    "  print('# of test examples: %d' % num_test_examples)\n",
    "\n",
    "\n",
    "def make_ngs_examples(hparams):\n",
    "  \"\"\"Generator function that yields training, evaluation and test examples.\"\"\"\n",
    "  ref_reader = fasta.IndexedFastaReader(input_path=hparams.ref_path)\n",
    "  vcf_reader = vcf.VcfReader(input_path=hparams.vcf_path)\n",
    "  read_requirements = reads_pb2.ReadRequirements()\n",
    "  sam_reader = sam.SamReader(\n",
    "      input_path=hparams.bam_path, read_requirements=read_requirements)\n",
    "\n",
    "  # Use a separate SAM reader to query for reads falling in the pileup range.\n",
    "  sam_query_reader = sam.SamReader(\n",
    "      input_path=hparams.bam_path, read_requirements=read_requirements)\n",
    "  used_pileup_ranges = set()\n",
    "  with ref_reader, vcf_reader, sam_reader, sam_query_reader:\n",
    "    for read in sam_reader:\n",
    "\n",
    "      # Check that read has cigar string present and allowed alignment.\n",
    "      if not read.alignment.cigar:\n",
    "        print('Skipping read, no cigar alignment found')\n",
    "        continue\n",
    "      if not has_allowed_alignment(read):\n",
    "        continue\n",
    "\n",
    "      # Obtain window that will be used to construct an example.\n",
    "      read_range = utils.read_range(read)\n",
    "      ref = ref_reader.query(region=read_range)\n",
    "      pileup_range = get_pileup_range(hparams, read, read_range, ref)\n",
    "\n",
    "      # Do not construct multiple examples with the same pileup range.\n",
    "      pileup_range_serialized = pileup_range.SerializeToString()\n",
    "      if pileup_range_serialized in used_pileup_ranges:\n",
    "        continue\n",
    "      used_pileup_ranges.add(pileup_range_serialized)\n",
    "\n",
    "      # Get reference sequence, reads, and truth variants for the pileup range.\n",
    "      pileup_reads = list(sam_query_reader.query(region=pileup_range))\n",
    "      pileup_ref = ref_reader.query(region=pileup_range)\n",
    "      pileup_variants = list(vcf_reader.query(region=pileup_range))\n",
    "      if is_usable_example(pileup_reads, pileup_variants, pileup_ref):\n",
    "        yield make_example(hparams, pileup_reads, pileup_ref, pileup_range)\n",
    "\n",
    "\n",
    "def get_pileup_range(hparams, read, read_range, ref):\n",
    "  \"\"\"Returns a range that will be used to construct one example.\"\"\"\n",
    "\n",
    "  # Find error positions where read and reference differ.\n",
    "  ngs_read_length = read_range.end - read_range.start\n",
    "  error_indices = [\n",
    "      i for i in range(ngs_read_length) if ref[i] != read.aligned_sequence[i]\n",
    "  ]\n",
    "\n",
    "  # If read and reference sequence are the same, create an example centered\n",
    "  # at middle base of read.\n",
    "  if not error_indices:\n",
    "    error_idx = ngs_read_length // 2\n",
    "\n",
    "  # If read and reference differ at one or more positions, create example\n",
    "  # centered at a random error position.\n",
    "  else:\n",
    "    error_idx = random.choice(error_indices)\n",
    "\n",
    "  error_pos = read_range.start + error_idx\n",
    "  flank_size = hparams.window_size // 2\n",
    "  return ranges.make_range(\n",
    "      chrom=read_range.reference_name,\n",
    "      start=error_pos - flank_size,\n",
    "      end=error_pos + flank_size + 1)\n",
    "\n",
    "\n",
    "def has_allowed_alignment(read):\n",
    "  \"\"\"Determines whether a read's CIGAR string has the allowed alignments.\"\"\"\n",
    "  return all([c.operation in _ALLOWED_CIGAR_OPS for c in read.alignment.cigar])\n",
    "\n",
    "\n",
    "def is_usable_example(reads, variants, ref_bases):\n",
    "  \"\"\"Determines whether a particular reference region and read can be used.\"\"\"\n",
    "  # Discard examples with variants or no mapped reads.\n",
    "  if variants or not reads:\n",
    "    return False\n",
    "\n",
    "  # Use only examples where all reads have simple alignment and allowed bases.\n",
    "  for read in reads:\n",
    "    if not has_allowed_alignment(read):\n",
    "      return False\n",
    "    if any(base not in _ALLOWED_BASES for base in read.aligned_sequence):\n",
    "      return False\n",
    "\n",
    "  # Reference should only contain allowed bases.\n",
    "  if any(base not in _ALLOWED_BASES for base in ref_bases):\n",
    "    return False\n",
    "  return True\n",
    "\n",
    "\n",
    "def make_example(hparams, pileup_reads, pileup_ref, pileup_range):\n",
    "  \"\"\"Takes in an input sequence and outputs tf.train.Example ProtocolMessages.\n",
    "\n",
    "  Each example contains the following features: A counts, C counts, G counts,\n",
    "  T counts, reference sequence, correct base label.\n",
    "  \"\"\"\n",
    "  assert len(pileup_ref) == hparams.window_size\n",
    "  example = tf.train.Example()\n",
    "  base_counts = np.zeros(shape=[hparams.window_size, len(_ALLOWED_BASES)])\n",
    "\n",
    "  for read in pileup_reads:\n",
    "    read_position = read.alignment.position.position\n",
    "    read_ints = [_ALLOWED_BASES.index(b) for b in read.aligned_sequence]\n",
    "    one_hot_read = np.zeros((len(read_ints), len(_ALLOWED_BASES)))\n",
    "    one_hot_read[np.arange(len(one_hot_read)), read_ints] = 1\n",
    "\n",
    "    window_start = read_position - pileup_range.start\n",
    "    window_end = window_start + len(read_ints)\n",
    "\n",
    "    # If read falls outside of window, adjust start/end indices for window.\n",
    "    window_start = max(0, window_start)\n",
    "    window_end = min(window_end, hparams.window_size)\n",
    "\n",
    "    # We consider four possible scenarios for each read and adjust start/end\n",
    "    # indices to only include portions of read that overlap the window.\n",
    "    # 1) Read extends past 5' end of window\n",
    "    # 2) Read extends past 3' end of window\n",
    "    # 3) Read extends past 5' and 3' ends of window\n",
    "    # 4) Read falls entirely within window\n",
    "    if window_start == 0 and window_end != hparams.window_size:\n",
    "      read_start = pileup_range.start - read_position\n",
    "      read_end = None\n",
    "    if window_end == hparams.window_size and window_start != 0:\n",
    "      read_start = None\n",
    "      read_end = -1 * ((read_position + len(read_ints)) - pileup_range.end)\n",
    "    if window_start == 0 and window_end == hparams.window_size:\n",
    "      read_start = pileup_range.start - read_position\n",
    "      read_end = read_start + hparams.window_size\n",
    "    if window_start != 0 and window_end != hparams.window_size:\n",
    "      read_start = None\n",
    "      read_end = None\n",
    "    base_counts[window_start:window_end] += one_hot_read[read_start:read_end]\n",
    "\n",
    "  # Use fractions at each position instead of raw base counts.\n",
    "  base_counts /= np.expand_dims(np.sum(base_counts, axis=-1), -1)\n",
    "\n",
    "  # Save counts/fractions for each base separately.\n",
    "  features = example.features\n",
    "  for i in range(len(_ALLOWED_BASES)):\n",
    "    key = '%s_counts' % _ALLOWED_BASES[i]\n",
    "    features.feature[key].float_list.value.extend(list(base_counts[:, i]))\n",
    "\n",
    "  features.feature['ref_sequence'].int64_list.value.extend(\n",
    "      [_ALLOWED_BASES.index(base) for base in pileup_ref])\n",
    "  flank_size = hparams.window_size // 2\n",
    "  true_base = pileup_ref[flank_size]\n",
    "  features.feature['label'].int64_list.value.append(\n",
    "      _ALLOWED_BASES.index(true_base))\n",
    "\n",
    "  return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(hparams, filename, num_epochs):\n",
    "  \"\"\"Reads in and processes the TFRecords dataset.\n",
    "\n",
    "  Builds a pipeline that returns pairs of features, label.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define field names, types, and sizes for TFRecords.\n",
    "  proto_features = {\n",
    "      'A_counts':\n",
    "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
    "      'C_counts':\n",
    "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
    "      'G_counts':\n",
    "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
    "      'T_counts':\n",
    "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
    "      'ref_sequence':\n",
    "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.int64),\n",
    "      'label':\n",
    "          tf.io.FixedLenFeature(shape=[1], dtype=tf.int64),\n",
    "  }\n",
    "\n",
    "  def _process_input(proto_string):\n",
    "    \"\"\"Helper function for input function that parses a serialized example.\"\"\"\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(\n",
    "        serialized=proto_string, features=proto_features)\n",
    "\n",
    "    # Stack counts/fractions for all bases to create input of dimensions\n",
    "    # `hparams.window_size` x len(_ALLOWED_BASES).\n",
    "    feature_columns = []\n",
    "    for base in _ALLOWED_BASES:\n",
    "      feature_columns.append(parsed_features['%s_counts' % base])\n",
    "    features = tf.stack(feature_columns, axis=-1)\n",
    "    label = parsed_features['label']\n",
    "    return features, label\n",
    "\n",
    "  ds = tf.data.TFRecordDataset(filenames=filename)\n",
    "  ds = ds.map(map_func=_process_input)\n",
    "  ds = ds.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "  ds = ds.batch(batch_size=hparams.batch_size).repeat(count=num_epochs)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseHparams(object):\n",
    "  \"\"\"Default hyperparameters.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               total_epochs=100,\n",
    "               learning_rate=0.004,\n",
    "               l2=0.001,\n",
    "               batch_size=256,\n",
    "               window_size=21,\n",
    "               ref_path='hs37d5.fa.gz',\n",
    "               vcf_path='NA12878_calls.vcf.gz',\n",
    "               bam_path='NA12878_sliced.bam',\n",
    "               out_dir='examples',\n",
    "               model_dir='ngs_model',\n",
    "               log_dir='logs'):\n",
    "\n",
    "    self.total_epochs = total_epochs\n",
    "    self.learning_rate = learning_rate\n",
    "    self.l2 = l2\n",
    "    self.batch_size = batch_size\n",
    "    self.window_size = window_size\n",
    "    self.ref_path = ref_path\n",
    "    self.vcf_path = vcf_path\n",
    "    self.bam_path = bam_path\n",
    "    self.out_dir = out_dir\n",
    "    self.model_dir = model_dir\n",
    "    self.log_dir = log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = BaseHparams()\n",
    "\n",
    "train_dataset = get_dataset(\n",
    "      hparams, filename=os.path.join(hparams.out_dir, _TRAIN), num_epochs=1)\n",
    "  eval_dataset = get_dataset(\n",
    "      hparams, filename=os.path.join(hparams.out_dir, _EVAL), num_epochs=1)\n",
    "  test_dataset = get_dataset(\n",
    "      hparams, filename=os.path.join(hparams.out_dir, _TEST), num_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f7273963dd9bfe9be0e1846ee9e3602811e6819f6350966f7cce8e14380187a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
